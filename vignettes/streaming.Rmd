---
title: "Live streaming tweets"
subtitle: "rtweet: Collecting Twitter Data"
output:
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Live streaming tweets}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE, eval = FALSE, comment = "#>", collapse = TRUE)
```

Prior to streaming, make sure to install and load rtweet. This
vignette assumes users have already setup user access tokens (see:
[obtaining and using access tokens](https://github.com/mkearney/rtweet/blob/master/vignettes/tokens.Rmd)).

```{r}
## Install rtweet
install.packages("rtweet")
## Load rtweet
library(rtweet)
```

### stream_tweets parameters

In addition to accessing Twitter's REST API (e.g., `search_tweets`,
`get_timeline`), rtweet makes it possible to capture live streams of
Twitter data using the `stream_tweets()` function. By default,
`stream_tweets` will stream for 30 seconds and return a random sample
of tweets. To modify the default settings, `stream_tweets` accepts
several parameters, including `q` (query used to filter tweets),
`timeout` (duration or time of stream), and `file_name` (path name for
saving raw json data).

```{r}
## Stream keywords used to filter tweets
q <- paste0("hillaryclinton,imwithher,realdonaldtrump,maga,electionday")

## Stream time in seconds so for one minute set timeout = 60
## For larger chunks of time, I recommend multiplying 60 by the number
## of desired minutes. This method scales up to hours as well
## (x * 60 = x mins, x * 60 * 60 = x hours)
## Stream for 30 minutes
streamtime <- 30 * 60

## Filename to save json data (backup)
filename <- "rtelect.json"
```

Once these parameters are specified, initiatve the stream (warning:
streaming will occupy your current instance of R until the stream has
been completed. It is possible to start a new instance; the stream
itself isn't very memory intensive though the parsing process that
follows often can be).

```{r}
## Stream election tweets
rt <- stream_tweets(q = q, timeout = streamtime, file_name = filename)
```

Parsing larger streams can take some time due to a lengthy simplifying
process used to convert the json data. For example, the stream above
yielded over 140,000 tweets and took my Macbook Air (4gb of
RAM) about 20 minutes.

```{r}
## Parse json file method
#rt <- stream_tweets(q = q, timeout = streamtime,
#                    parse = FALSE, file_name = filename)
#rt <- parse_stream("rtweet_elxn.json")

## Preview tweets data
head(rt)

## Preview users data
users_data(rt) %>%
     head()
```

```{r, eval=TRUE, echo=FALSE}
##rt <- readRDS("vignettes/files/electweets.rds")
```
Once parsed, use `ts_plot` provides a quick visual of the frequency of
tweets. By default, `ts_plot` will try to aggregate time by the
day. Because I know the stream only lasted 30 minutes, I've opted to
aggregate tweets by the second. It'd also be possible to aggregate by
the minute, i.e., `by = "mins"`, or by some value of seconds, e.g.,
`by = "15 secs"`. I usually fiddle around with this a bit until the
plot becomes clearer.

```{r, eval=FALSE}
## Plot time series of all tweets aggregated by second
ts_plot(rt, by = "secs")
```

![stream-ts.png](files/stream-ts.png)

```{r, eval=FALSE}
## plot multiple time series by first filtering the data using
## regular expressions on the tweet "text" variable
rt %>%
    ts_filter(
        by = "mins",
        filter = c("hillary|clinton|imwithher",
                   "donald|trump|maga",
                   "vot|democracy"),
        key = c("Clinton", "Trump", "Democracy"),
        trim = TRUE) %>%
    ## The pipe operator allows you to combine this with ts_plot
    ## without things getting too messy.
    ts_plot(
        theme = "spacegray",
        cols = c("#6699ee", "#dd7a7a", "#7acc7a"),
        main = "Tweets during election day for the 2016 U.S. election",
        subtitle = "Tweets collected, parsed, and plotted using `rtweet`")
```

Often times these plots will take the shape of a frowny face with the
first and last points appearing significantly lower than the
rest. This is because the first and last time intervas are
artificially shrunken by connection and disconnection processes. To
remedy this, I specified `trim = TRUE` in the `ts_filter` example
above, which tells R to drop the first and last observation for each
time series, yielding a more attractive looking plot.

![stream-filter.png](files/stream-filter.png)
