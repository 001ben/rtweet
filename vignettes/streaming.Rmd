---
title: "Live streaming tweets"
subtitle: "rtweet: Collecting Twitter Data"
output:
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Live streaming tweets}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE, eval = FALSE, comment = "#>", collapse = TRUE)
```

## Live stream tweets using stream_tweets()

Prior to streaming, make sure to install and load rtweet. This
vignette assumes users have already setup user access tokens (see:
[obtaining and using access tokens](https://github.com/mkearney/rtweet/blob/master/vignettes/tokens.Rmd)).

```{r}
## Load rtweet
library(rtweet)
```

### stream_tweets parameters

In addition to accessing Twitter's REST API (e.g., `search_tweets`,
`get_timeline`), rtweet makes it possible to capture live streams of
Twitter data using the `stream_tweets()` function. By default,
`stream_tweets` will stream for 30 seconds and return a random sample
of tweets. To modify defaults, `stream_tweets` accepts several
parameters, including `q` (filter query), `timeout` (duration or time
of stream), and `file_name` (path name for saving raw json data).

```{r}
## Stream keywords used to filter tweets
q <- paste0("hillaryclinton,imwithher,realdonaldtrump,maga,electionday")

## Stream time in seconds (x * 60 = x mins, x * 60 * 60 = x hours)
## Stream for 30 minutes
streamtime <- 30 * 60

## Filename to save json data (backup)
filename <- "rtelect.json"
```

Once these parameters are specified, initiatve the stream (warning:
streaming will occupy your current instance of R until the stream has
been completed. It is possible to start a new instance; the stream
itself isn't very memory intensive though the parsing process that
follows often can be).

```{r}
## Stream election tweets
rt <- stream_tweets(q = q, timeout = streamtime, file_name = filename)
```

Parsing larger streams can take some time due to a lengthy simplifying
process used to convert the json data. For example, the stream above
yielded over 140,000 tweets and took my Macbook Air (4gb of
RAM) about 20 minutes.

```{r}
## Parse json file method
#rt <- stream_tweets(q = q, timeout = streamtime,
#                    parse = FALSE, file_name = filename)
#rt <- parse_stream("rtweet_elxn.json")

## Preview tweets data
head(rt)

## Preview users data
users_data(rt) %>%
     head()
```

```{r, eval=TRUE, echo=FALSE}
##rt <- readRDS("vignettes/files/electweets.rds")
```

```{r, eval=FALSE}
## Plot time series of all tweets
ts_plot(rt, "secs")
```

![stream-ts.png](files/stream-ts.png)

```{r, eval=FALSE}
## Return ggplot2 object (must have ggplot2 installed)
rt %>%
    ts_filter(
        by = "mins",
        filter = c("hillary|clinton|imwithher",
                   "donald|trump|maga",
                   "vot|democracy"),
        key = c("Clinton", "Trump", "Democracy"),
        trim = TRUE) %>%
    ts_plot(
        theme = "spacegray", cols = c("#6699ee", "#dd7a7a", "#7acc7a"),
        main = "Tweets during election day for the 2016 U.S. election",
        subtitle = "Tweets collected, parsed, and plotted using `rtweet`")
```

![stream-filter.png](files/stream-filter.png)
