#' search_tweets
#'
#' @description Returns two data frames (tweets data and users data)
#'   using a provided search query.
#'
#' @param q Query to be searched, used in filtering relevant tweets
#'   to return from Twitter's REST API. Should be a character
#'   string not to exceed 500 characters maximum. Spaces are assumed
#'   to function like boolean "AND" operators. To search for tweets
#'   including one of multiple possible terms, separate search terms
#'   with spaces and the word "OR". For example, the search
#'   \code{query = "data science"} searches for tweets using both
#'   "data" and "science" though the words can appear anywhere and
#'   in any order in the tweet. However, when OR is added between
#'   search terms, \code{query = "data OR science"}, Twitter's REST
#'   API should return any tweet that includes either "data" or
#'   "science" appearing in the tweets. It is also possible to search
#'   for exact phrases uses double quotations. To do this, either
#'   wrap single quotes around a search including double quotes, e.g.,
#'   \code{q = '"data science"'} or escape internal double quotes
#'   using a single backslash, e.g., \code{q = "\"data science\""}.
#' @param n Numeric, specifying the total number of desired tweets to
#'   return. Defaults to 100. Maximum number of tweets returned from
#'   a single token is 18,000. To return more than 18,000 tweets, users
#'   are encourages to set \code{retryonratelimit} to TRUE. See details
#'   for more information.
#' @param type Character string specifying which type of search
#'   results to return from the REST API. The current default is
#'   \code{type = "recent"}, other valid types include
#'   \code{type = "mixed"} and \code{type = "popular"}.
#' @param include_rts Logical, indicating whether to include retweets
#'   in search results. Retweets are classified as any tweet
#'   generated by Twitter's built-in "retweet" (recycle arrows)
#'   function. These are distinct from quotes (retweets with
#'   additional text provided from sender) or manual retweets
#'   (old school method of manually entering "RT" into the text
#'   of one's tweets).
#' @param max_id Character string specifying the [oldest] status
#'   id beyond which search results should resume returning.
#'   Especially useful large data returns that require multiple
#'   iterations interrupted by user time constraints. For searches
#'   exceeding 18,000 tweets, users are encouraged to take advantage
#'   of rtweet's internal automation procedures for waiting on
#'   rate limits via setting \code{retryonratelimit} to TRUE.
#'   However, due to processing time and rate limits, retreiving
#'   several million tweets can take several hours or multiple days.
#'   In these situations, it may be useful to leverage
#'   \code{retryonratelimit} for chunks, using the \code{max_id}
#'   parameter to allow one search to continue where the previous
#'   search left off.
#' @param usr Logical indicating whether to return users data frame.
#'   Defaults to true, see \code{\link{users_data}}.
#' @param parse Logical, indicating whether to return parsed
#'   (data.frames) or nested list (fromJSON) object. By default,
#'   \code{parse = TRUE} saves users from the time
#'   [and frustrations] associated with disentangling the Twitter
#'   API return objects.
#' @param token OAuth token. By default \code{token = NULL} fetches a
#'   non-exhausted token from an environment variable. Find
#'   instructions on how to create tokens and setup an environment
#'   variable in the tokens vignette (in r, send \code{?tokens} to
#'   console).
#' @param retryonratelimit Logical indicating whether to wait and
#'   retry when rate limited. This argument is only relevant if the
#'   desired return (n) exceeds the remaining limit of available
#'   requests. Defaults to false. Set this value to TRUE to automate
#'   large searches (i.e., n > 18000). For many searches, esp.
#'   specific or specialized searches, there won't be more than
#'   18,000 tweets to return. But for broad, generic, or popular
#'   topics, the total number of tweets within the REST window of
#'   time (7-10 days) can easily reach the millions.
#' @param verbose Logical, indicating whether or not to include
#'   output processing/retrieval messages. Defaults to TRUE. For
#'   larger searches, messages include rough estimates for time
#'   remaining between searches. It should be noted, however, that
#'   these time estimates only describe the amount of time between
#'   searches and not the total time remaining. For large searches
#'   conducted with \code{retryonratelimit} set to TRUE, the
#'   estimated retreival time can be estimated by dividing the number
#'   of requested tweets by 18,000 and then multiplying the quotient
#'   by 15 (token cooldown time, in minutes).
#' @param \dots Futher arguments passed on to \code{make_url}.
#'   All named arguments that do not match the above arguments
#'   (i.e., count, type, etc.) will be built into the request.
#'   To return only English language tweets, for example, use
#'   \code{lang = "en"}. For more options see Twitter's
#'   API documentation.
#' @seealso \url{https://dev.twitter.com/overview/documentation}
#' @details Twitter API document recommends limiting searches to
#'   10 keywords and operators. Complex queries may also produce
#'   API errors preventing recovery of information related to
#'   the query.
#'   It should also be noted Twitter's search API does not consist
#'   of an index of all Tweets. At the time of searching, the
#'   search API index includes between only 6-9 days of Tweets.
#'
#'
#'   Number of tweets returned will often be less than what was
#'   specified by the user. This can happen because (a) the search
#'   query did not return many results (the search pool is already
#'   thinned out from the population of tweets to begin with) or
#'   (b) because you hit your rate limit for a given token. Even if
#'   the query has lots of hits and the rate limit should be able to
#'   max out at 18,000, the returned number of tweets may be lower,
#'   but that's only because the functions filter out duplicates
#'   (e.g., 18,000 tweets were actually returned, but 30 of them were
#'   removed because they were repeats).
#' @examples
#' \dontrun{
#' # search for 1000 tweets mentioning Hillary Clinton
#' hrc <- search_tweets(q = "hillaryclinton", n = 1000)
#'
#' # data frame where each observation (row) is a different tweet
#' hrc
#'
#' # users data also retrieved. can access it via users_data()
#' users_data(hrc)
#'
#' # search for 1000 tweets in English
#' djt <- search_tweets(q = "realdonaldtrump", n = 1000, lang = "en")
#' djt # prints tweets data preview
#' users_data(djt) # prints users data preview
#' }
#' @return List object with tweets and users each returned as a
#'   data frame.
#' @family tweets
#' @export
search_tweets <- function(q, n = 100,
                          type = "recent",
                          max_id = NULL,
                          include_rts = TRUE,
                          parse = TRUE,
                          usr = TRUE,
                          token = NULL,
                          retryonratelimit = FALSE,
                          verbose = TRUE,
                          ...) {

    ## check token and get rate limit data
    token <- check_token(token, "search/tweets")
    rtlimit <- rate_limit(token, "search/tweets")
    remaining <- rtlimit[["remaining"]] * 100
    reset <- rtlimit[["reset"]]
    units(reset) <- "mins"

    if (any(n <= remaining, !retryonratelimit)) {
        rt <- .search_tweets(
            q = q, n = n,
            check = check,
            type = type,
            max_id = max_id,
            include_rts = include_rts,
            parse = parse,
            usr = usr,
            token = token,
            verbose = verbose, ...)
    } else {
        if (identical(remaining, 0)) {
            ntimes <- ceiling((n - remaining) / 18000)
        } else {
            ntimes <- ceiling((n - remaining) / 18000) + 1
        }
        rt <- vector("list", ntimes)
        maxid <- max_id

        for (i in seq_len(ntimes)) {
            ## if rate limited (exhausted token)
            if (identical(remaining, 0)) {
                message(paste0(
                    "retry on rate limit...\n",
                    "waiting about ",
                    round(reset, 0),
                    " minutes..."))
                Sys.sleep(as.double(reset) * 60 + 1)
                remaining <- 180 * 100
            }
            rt[[i]] <- tryCatch(
                .search_tweets(
                    q = q, n = remaining,
                    check = FALSE,
                    type = type,
                    max_id = maxid,
                    include_rts = include_rts,
                    parse = parse,
                    usr = usr,
                    token = token,
                    verbose = verbose, ...),
                error = function(e) return(NULL))
            ## break if error
            if (is.null(rt[[i]])) break
            ## break if final i
            if (i == ntimes) break
            ## get next maxid
            maxid.new <- rt[[i]][["status_id"]][[NROW(rt[[i]])]]
            ## break if new maxid is null, empty, or unchanged
            if (any(is.null(maxid.new),
                    identical(length(maxid.new), 0L),
                    identical(maxid, maxid.new))) break
            ## update maxid value
            maxid <- maxid.new
            ## refresh rate limit data
            rtlimit <- rate_limit(token, "search/tweets")
            remaining <- rtlimit[["remaining"]] * 100
            reset <- rtlimit[["reset"]]
            units(reset) <- "mins"
        }
        ## get users data if applicable
        if (usr) {
            users <- do.call("rbind", users_data(rt))
            rt <- do.call("rbind", rt)
            attr(rt, "users") <- users
        } else {
            rt <- do.call("rbind", rt)
        }
    }
    rt
}


.search_tweets <- function(q, n = 100,
                          check = FALSE,
                          type = "recent",
                          max_id = NULL,
                          include_rts = TRUE,
                          parse = TRUE,
                          usr = TRUE,
                          token = NULL,
                          verbose = TRUE, ...) {

    query <- "search/tweets"
    stopifnot(is_n(n), is.atomic(q), is.atomic(max_id))
    #token <- check_token(token, query)

    if (check) {
        n.times <- rate_limit(token, query)[["remaining"]]
    } else {
        n.times <- 180
    }

    if (nchar(q) > 500) {
        stop("q cannot exceed 500 characters.", call. = FALSE)
    }

    if (length(type) > 1) {
        stop("can only select one search type. Try type = 'recent'.",
             call. = FALSE)
    }

    if (!isTRUE(tolower(type) %in% c("mixed", "recent", "popular"))) {
        stop("invalid search type - must be mixed, recent, or popular.",
             call. = FALSE)
    }

    if (!include_rts) q <- paste0(q, " -filter:retweets")

    params <- list(q = q,
                   result_type = type,
                   count = 100,
                   max_id = max_id,
                   ...)

    url <- make_url(
        query = query,
        param = params)

    if (verbose) message("Searching for tweets...")

    tw <- scroller(url, n, n.times, type = "search", token)

    if (parse) {
        tw <- parse.piper(tw, usr = usr)
    }

    if (verbose) {
        message("Finished collecting tweets!")
    }
    tw
}


#' search_users
#'
#' @description Returns data frame of users data using a provided
#'   search query.
#'
#' @param q Query to be searched, used in filtering relevant tweets
#'   to return from Twitter's REST API. Should be a character
#'   string not to exceed 500 characters maximum. Spaces are assumed
#'   to function like boolean "AND" operators. To search for tweets
#'   including one of multiple possible terms, separate search terms
#'   with spaces and the word "OR". For example, the search
#'   \code{query = "data science"} searches for tweets using both
#'   "data" and "science" though the words can appear anywhere and
#'   in any order in the tweet. However, when OR is added between
#'   search terms, \code{query = "data OR science"}, Twitter's REST
#'   API should return any tweet that includes either "data" or
#'   "science" appearing in the tweets. At this time, Twitter's users/search
#'   API does not allow complex searches or queries targetting exact phrases
#'   as is allowed by \code{search_tweets}.
#' @param n Numeric, specifying the total number of desired users to
#'   return. Defaults to 100. Maximum number of users returned from
#'   a single search is 1,000.
#' @param parse Logical, indicating whether to return parsed
#'   (data.frames) or nested list (fromJSON) object. By default,
#'   \code{parse = TRUE} saves users from the time
#'   [and frustrations] associated with disentangling the Twitter
#'   API return objects.
#' @param tw Logical indicating whether to return tweets data frame.
#'   Defaults to true.
#' @param clean_tweets logical indicating whether to remove non-ASCII
#'   characters in text of tweets. defaults to FALSE.
#' @param as_double logical indicating whether to handle ID variables
#'   as double (numeric) class. By default, this is set to FALSE, meaning
#'   ID variables are treated as character vectors. Setting this to
#'   TRUE can provide performance (speed and memory) boost but can also
#'   lead to issues when printing and saving, depending on the format.
#' @param token OAuth token. By default \code{token = NULL} fetches a
#'   non-exhausted token from an environment variable. Find instructions
#'   on how to create tokens and setup an environment variable in the
#'   tokens vignette (in r, send \code{?tokens} to console).
#' @param verbose Logical, indicating whether or not to output
#'   processing/retrieval messages.
#' @seealso \url{https://dev.twitter.com/overview/documentation}
#' @examples
#' \dontrun{
#' # search for 1000 tweets mentioning Hillary Clinton
#' pc <- search_users(q = "political communication", n = 1000)
#'
#' # data frame where each observation (row) is a different user
#' pc
#'
#' # tweets data also retrieved. can access it via tweets_data()
#' users_data(hrc)
#' }
#' @return Data frame of users returned by query.
#' @family users
#' @export
search_users <- function(q, n = 20,
                         parse = TRUE,
                         tw = TRUE,
                         clean_tweets = FALSE,
                         as_double = FALSE,
                         token = NULL,
                         verbose = TRUE) {

    query <- "users/search"
    stopifnot(is_n(n), is.atomic(q))
    token <- check_token(token, query)
    if (n > 1000) {
        warning(
            paste0("search only returns up to 1,000 users per ",
                   "unique search. Setting n to 1000..."))
        n <- 1000
    }
    n.times <- ceiling(n / 20)
    if (n.times > 50) n.times <- 50

    if (nchar(q) > 500) {
        stop("q cannot exceed 500 characters.", call. = FALSE)
    }

    params <- list(q = q,
                   count = 20,
                   page = 1)
    url <- make_url(
        query = query,
        param = params)
    if (verbose) message("Searching for users...")

    usr <- vector("list", n.times)
    k <- 0
    nrows <- NULL

    for (i in seq_len(n.times)) {
        r <- tryCatch(
            TWIT(get = TRUE, url, token),
            error = function(e) return(NULL))

        if (is.null(r)) break

        usr[[i]] <- from_js(r)

        if (identical(length(usr[[i]]), 0)) break
        if (isTRUE(is.numeric(NROW(usr[[i]])))) {
            nrows <- NROW(usr[[i]])
        } else {
            if (identical(nrows, 0)) break
            nrows <- 0
        }
        k <- k + nrows
        if (k >= n) break
        url$query$page <- (i + 1L)
    }
    if (parse) {
        usr <- parse.piper.usr(usr, tw = tw)
    }
    if (verbose) {
        message("Finished collecting users!")
    }
    usr
}

count_users_returned <- function(x) {
    length(unique(unlist(lapply(x, function(x) x[["id_str"]]),
                         use.names = FALSE)))
}


#' Get value for max_id
#'
#' @param df Tweets data frame with "created_at" and "status_id" variables.
#'
#' @return Character string of max_id to be used in future function calls.
#' @export
next_id <- function(df) {
    if (!all(c("created_at", "status_id") %in% names(df))) {
        stop("wrong data frame - function requires tweets data")
    }
    if (any(grepl("posix", class(df$created_at),
                  ignore.case = TRUE))) {
        df$created_at <- format_date(df$created_at)
    }
    df <- df[!is.na(df$status_id), ]
    df <- df[order(df$created_at), ]

    return(df$status_id[1])
}
